{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "# %tensorflow_version 1.14\n",
    "from keras.layers import Dense, Flatten, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import SARSAAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.callbacks import TrainEpisodeLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "from keras.callbacks import History\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Lambda\n",
    "import keras.backend as K\n",
    "\n",
    "from rl.core import Agent\n",
    "from rl.agents.dqn import mean_q\n",
    "from rl.util import huber_loss\n",
    "from rl.policy import EpsGreedyQPolicy, GreedyQPolicy\n",
    "from rl.util import get_object_config\n",
    "\n",
    "\n",
    "class SARSAAgent(Agent):\n",
    "    \"\"\"This class defines the SARSA agent\n",
    "    \"\"\"\n",
    "    def __init__(self, model, nb_actions, policy, test_policy=EpsGreedyQPolicy(), gamma=.9, nb_steps_warmup=10,\n",
    "                 train_interval=1, delta_clip=np.inf, *args, **kwargs):\n",
    "        super(SarsaAgent, self).__init__(*args, **kwargs)\n",
    "\n",
    "        if policy is None:\n",
    "            policy = EpsGreedyQPolicy()\n",
    "        if test_policy is None:\n",
    "            test_policy = GreedyQPolicy()\n",
    "\n",
    "        self.model = model\n",
    "        self.nb_actions = nb_actions\n",
    "        self.policy = policy\n",
    "        self.test_policy = test_policy\n",
    "        self.gamma = gamma\n",
    "        self.nb_steps_warmup = nb_steps_warmup\n",
    "        self.train_interval = train_interval\n",
    "\n",
    "        self.delta_clip = delta_clip\n",
    "        self.compiled = False\n",
    "        self.actions = None\n",
    "        self.observations = None\n",
    "        self.rewards = None\n",
    "        self.q_values=[]\n",
    "\n",
    "    def compute_batch_q_values(self, state_batch):\n",
    "        batch = self.process_state_batch(state_batch)\n",
    "        q_values = self.model.predict_on_batch(batch)\n",
    "        assert q_values.shape == (len(state_batch), self.nb_actions)\n",
    "        return q_values\n",
    "\n",
    "    def compute_q_values(self, state):\n",
    "        q_values = self.compute_batch_q_values([state]).flatten()\n",
    "        assert q_values.shape == (self.nb_actions,)\n",
    "        return q_values\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        batch = np.array(batch)\n",
    "        if self.processor is None:\n",
    "            return batch\n",
    "        return self.processor.process_state_batch(batch)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SarsaAgent, self).get_config()\n",
    "        config['nb_actions'] = self.nb_actions\n",
    "        config['gamma'] = self.gamma\n",
    "        config['nb_steps_warmup'] = self.nb_steps_warmup\n",
    "        config['train_interval'] = self.train_interval\n",
    "        config['delta_clip'] = self.delta_clip\n",
    "        config['model'] = get_object_config(self.model)\n",
    "        config['policy'] = get_object_config(self.policy)\n",
    "        config['test_policy'] = get_object_config(self.test_policy)\n",
    "        return config\n",
    "\n",
    "    def compile(self, optimizer, metrics=[]):\n",
    "        metrics += [mean_q]  \n",
    "\n",
    "        def clipped_masked_error(args):\n",
    "            y_true, y_pred, mask = args\n",
    "            loss = huber_loss(y_true, y_pred, self.delta_clip)\n",
    "            loss *= mask  \n",
    "            return K.sum(loss, axis=-1)\n",
    "\n",
    "        ### Creating trainable model. \n",
    "\n",
    "        # The problem is that we need to mask the output since we only\n",
    "        # ever want to update the Q values for a certain action. The way we achieve this is by\n",
    "        # using a custom Lambda layer that computes the loss. This gives us the necessary flexibility\n",
    "        # to mask out certain parameters by passing in multiple inputs to the Lambda layer.\n",
    "\n",
    "        y_pred = self.model.output\n",
    "        y_true = Input(name='y_true', shape=(self.nb_actions,))\n",
    "        mask = Input(name='mask', shape=(self.nb_actions,))\n",
    "        loss_out = Lambda(clipped_masked_error, output_shape=(1,), name='loss')([y_pred, y_true, mask])\n",
    "        ins = [self.model.input] if type(self.model.input) is not list else self.model.input\n",
    "        trainable_model = Model(inputs=ins + [y_true, mask], outputs=[loss_out, y_pred])\n",
    "        assert len(trainable_model.output_names) == 2\n",
    "        combined_metrics = {trainable_model.output_names[1]: metrics}\n",
    "        losses = [\n",
    "            lambda y_true, y_pred: y_pred,  # loss is computed in Lambda layer\n",
    "            lambda y_true, y_pred: K.zeros_like(y_pred),  # we only include this for the metrics\n",
    "        ]\n",
    "        trainable_model.compile(optimizer=optimizer, loss=losses, metrics=combined_metrics)\n",
    "        self.trainable_model = trainable_model\n",
    "\n",
    "        self.compiled = True\n",
    "\n",
    "    def load_weights(self, filepath):\n",
    "        self.model.load_weights(filepath)\n",
    "\n",
    "    def save_weights(self, filepath, overwrite=False):\n",
    "        self.model.save_weights(filepath, overwrite=overwrite)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.actions = collections.deque(maxlen=2)\n",
    "        self.observations = collections.deque(maxlen=2)\n",
    "        self.rewards = collections.deque(maxlen=2)\n",
    "        if self.compiled:\n",
    "            self.model.reset_states()\n",
    "\n",
    "    def forward(self, observation):\n",
    "        # Select an action.\n",
    "        q_values = self.compute_q_values([observation])\n",
    "        if self.training:\n",
    "            action = self.policy.select_action(q_values=q_values)\n",
    "        else:\n",
    "            action = self.test_policy.select_action(q_values=q_values)\n",
    "\n",
    "        # Book-keeping.\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "        # self.q_values=q_values\n",
    "\n",
    "        return action\n",
    "\n",
    "    def backward(self, reward, terminal):\n",
    "        metrics = [np.nan for _ in self.metrics_names]\n",
    "        if not self.training:\n",
    "            # We're done here. No need to update the experience memory since we only use the working\n",
    "            # memory to obtain the state over the most recent observations.\n",
    "            return metrics\n",
    "\n",
    "        # Train the network on a single stochastic batch.\n",
    "        \n",
    "        if self.step > self.nb_steps_warmup and self.step % self.train_interval == 0:\n",
    "            # Start by extracting the necessary parameters (we use a vectorized implementation).\n",
    "            self.rewards.append(reward)\n",
    "            if len(self.observations) < 2:\n",
    "                return metrics  # not enough data yet\n",
    "\n",
    "            state0_batch = [self.observations[0]]\n",
    "            reward_batch = [self.rewards[0]]\n",
    "            action_batch = [self.actions[0]]\n",
    "            terminal1_batch = [0.] if terminal else [1.]\n",
    "            state1_batch = [self.observations[1]]\n",
    "            action1_batch = [self.actions[1]]\n",
    "\n",
    "            # Prepare and validate parameters.\n",
    "            state0_batch = self.process_state_batch(state0_batch)\n",
    "            state1_batch = self.process_state_batch(state1_batch)\n",
    "            terminal1_batch = np.array(terminal1_batch)\n",
    "            reward_batch = np.array(reward_batch)\n",
    "            assert reward_batch.shape == (1,)\n",
    "            assert terminal1_batch.shape == reward_batch.shape\n",
    "            assert len(action_batch) == len(reward_batch)\n",
    "\n",
    "            batch = self.process_state_batch(state1_batch)\n",
    "            q_values = self.compute_q_values(batch)\n",
    "            q_values = q_values.reshape((1, self.nb_actions))\n",
    "            probs=q_values[0]\n",
    "            probs/=np.sum(probs)\n",
    "            self.q_values.append(probs)\n",
    "            # self.q_values/=np.sum(q_values)\n",
    "\n",
    "            q_batch = q_values[0, action1_batch]\n",
    "\n",
    "            assert q_batch.shape == (1,)\n",
    "            targets = np.zeros((1, self.nb_actions))\n",
    "            dummy_targets = np.zeros((1,))\n",
    "            masks = np.zeros((1, self.nb_actions))\n",
    "\n",
    "            # Compute r_t + gamma * Q(s_t+1, a_t+1)\n",
    "            discounted_reward_batch = self.gamma * q_batch\n",
    "            # Set discounted reward to zero for all states that were terminal.\n",
    "            discounted_reward_batch *= terminal1_batch\n",
    "            assert discounted_reward_batch.shape == reward_batch.shape\n",
    "            Rs = reward_batch + discounted_reward_batch\n",
    "            for idx, (target, mask, R, action) in enumerate(zip(targets, masks, Rs, action_batch)):\n",
    "                target[action] = R  # update action with estimated accumulated reward\n",
    "                dummy_targets[idx] = R\n",
    "                mask[action] = 1.  # enable loss for this specific action\n",
    "            targets = np.array(targets).astype('float32')\n",
    "            masks = np.array(masks).astype('float32')\n",
    "\n",
    "            # Finally, perform a single update on the entire batch. We use a dummy target since\n",
    "            # the actual loss is computed in a Lambda layer that needs more complex input. However,\n",
    "            # it is still useful to know the actual target to compute metrics properly.\n",
    "            state0_batch = state0_batch.reshape((1,) + state0_batch.shape)\n",
    "            ins = [state0_batch] if type(self.model.input) is not list else state0_batch\n",
    "            metrics = self.trainable_model.train_on_batch(ins + [targets, masks], [dummy_targets, targets])\n",
    "            metrics = [metric for idx, metric in enumerate(metrics) if idx not in (1, 2)]  # throw away individual losses\n",
    "            metrics += self.policy.metrics\n",
    "            if self.processor is not None:\n",
    "                metrics += self.processor.metrics\n",
    "        return metrics\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return self.model.layers[:]\n",
    "\n",
    "    @property\n",
    "    def metrics_names(self):\n",
    "        # Throw away individual losses and replace output name since this is hidden from the user.\n",
    "        assert len(self.trainable_model.output_names) == 2\n",
    "        dummy_output_name = self.trainable_model.output_names[1]\n",
    "        model_metrics = [name for idx, name in enumerate(self.trainable_model.metrics_names) if idx not in (1, 2)]\n",
    "        model_metrics = [name.replace(dummy_output_name + '_', '') for name in model_metrics]\n",
    "\n",
    "        names = model_metrics + self.policy.metrics_names[:]\n",
    "        if self.processor is not None:\n",
    "            names += self.processor.metrics_names[:]\n",
    "        return names\n",
    "\n",
    "    @property\n",
    "    def policy(self):\n",
    "        return self.__policy\n",
    "\n",
    "    @policy.setter\n",
    "    def policy(self, policy):\n",
    "        self.__policy = policy\n",
    "        self.__policy._set_agent(self)\n",
    "\n",
    "    @property\n",
    "    def test_policy(self):\n",
    "        return self.__test_policy\n",
    "\n",
    "    @test_policy.setter\n",
    "    def test_policy(self, policy):\n",
    "        self.__test_policy = policy\n",
    "        self.__test_policy._set_agent(self)\n",
    "\n",
    "# Aliases\n",
    "SarsaAgent = SARSAAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the environment\n",
    "env = gym.make('CartPole-v0')\n",
    "seed_val = 456\n",
    "env.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the state and action space\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n\n",
    "\n",
    "#Defining a Neural Network function for our Cartpole agent \n",
    "def agent(states, actions):\n",
    "    \"\"\"Creating a simple Deep Neural Network.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape = (1, states)))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting our neural network\n",
    "model = agent(states, actions)\n",
    "#Defining SARSA Keras-RL agent: inputing the policy and the model\n",
    "sarsa = SARSAAgent(model=model, nb_actions=actions, policy=EpsGreedyQPolicy())\n",
    "#Compiling SARSA with mean squared error loss\n",
    "sarsa.compile('adam', metrics=[\"mse\"])\n",
    "\n",
    "#Training the agent for 50000 steps\n",
    "sarsa.fit(env, nb_steps=500, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting and testing our agent model for 500 episodes\n",
    "scores = sarsa.test(env, nb_episodes = 500, visualize= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "#plotting and visualisation \n",
    "statistics = scores.history\n",
    "fig = go.Figure([go.Scatter(x=statistics['episode_reward'],y =statistics['nb_steps'])])\n",
    "fig.show()\n",
    "\n",
    "#retaining the action probabilities \n",
    "q_values = np.array(sarsa.q_values)\n",
    "print(\"the action probabilities are\", q_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the weights\n",
    "sarsa.save_weights('sarsa_{}_weights.h5f'.format(env), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = scores.history\n",
    "\n",
    "episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
