{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class eps_greedy:\n",
    "    \n",
    "    def __init__(self, k, ietrs, eps, mu='random'):\n",
    "        #number of arms\n",
    "        self.k = k\n",
    "        #eps value (epsilon probability with whivh the actions are selected)\n",
    "        self.eps = eps\n",
    "        #number of iterations\n",
    "        self.iters = iters \n",
    "        #step counts\n",
    "        self.n = 0\n",
    "        #step counts for k arms\n",
    "        self.k_n = np.zeros(k)\n",
    "        #mean reward\n",
    "        self.mean_reward = 0\n",
    "        self.reward = np.zeros(iters)\n",
    "        #rewars for k arms\n",
    "        self.k_reward = np.zeros(k)\n",
    "\n",
    "    if type(mu) == list:\n",
    "        self.mu = np.array(mu)\n",
    "        \n",
    "    elif mu == 'random':\n",
    "        self.mu = np.random.normal(0,1,k)\n",
    "        \n",
    "    elif mu == 'sequence':\n",
    "        self.mu = np.linspace(0,k-1,k)\n",
    "    \n",
    "    #function for probability calculations \n",
    "    def calculate(self):\n",
    "        \n",
    "        p = np.random.rand()\n",
    "        \n",
    "        if self.eps ==0 and self.n ==0:\n",
    "            \n",
    "            a = np.random.choice(self.k)\n",
    "            \n",
    "        elif p < self.eps:\n",
    "            \n",
    "            a = np.random.choice(self.k)\n",
    "            \n",
    "        else: \n",
    "            \n",
    "            \"\"\"\n",
    "            Qt​(a)=E[Rn​∣An​=a]\n",
    "            \n",
    "            action selection is goverened by above formula where \n",
    "            the estimated action value Q is calculated and action A is \n",
    "            chosen by using argmax function given below.\n",
    "            \n",
    "            \n",
    "            A = argmax(Qt(a))\n",
    "                    a\n",
    "\n",
    "            \"\"\"\n",
    "            a = np.argmax(self.k_reward)\n",
    "            \n",
    "            \"\"\"\n",
    "            reward is nothing but normal distribution of natural numbers \n",
    "        \n",
    "            \"\"\"\n",
    "            reward = np.random.random(self.mu[a], 1)\n",
    "            \n",
    "            #update\n",
    "            \n",
    "            self.n +=1\n",
    "            self.k_n[a] += 1\n",
    "            \n",
    "            #update the mean formula\n",
    "            \n",
    "            self.mean_reward = self.mean_reward + (reward - self.mean_reward)/self.n\n",
    "            \n",
    "            \n",
    "            self.k_reward[a] = self.k_reward[a] + (reward - self.k_reward[a])/ self.k_n[a]\n",
    "            \n",
    "            \n",
    "            def calculate(self):\n",
    "                \n",
    "                for i in range(self.iters):\n",
    "                    \n",
    "                    self.calculate()\n",
    "                    self.reward[i] = self.mean_reward \n",
    "                    \n",
    "                    \n",
    "            def reset(self):\n",
    "        # Resets results while keeping settings\n",
    "                self.n = 0\n",
    "                self.k_n = np.zeros(k)\n",
    "                self.mean_reward = 0\n",
    "                self.reward = np.zeros(iters)\n",
    "                self.k_reward = np.zeros(k)\n",
    "                \n",
    "                \n",
    "                k = 10\n",
    "                iters = 1000\n",
    "\n",
    "                eps_0_rewards = np.zeros(iters)\n",
    "                eps_01_rewards = np.zeros(iters)\n",
    "                eps_1_rewards = np.zeros(iters)\n",
    "                eps_0_selection = np.zeros(k)\n",
    "                eps_01_selection = np.zeros(k)\n",
    "                eps_1_selection = np.zeros(k)\n",
    "\n",
    "                episodes = 1000\n",
    "                \n",
    "                # Run experiments\n",
    "                for i in range(episodes):\n",
    "                    \n",
    "                    # Initialize bandits\n",
    "                    eps_0 = eps_greedy(k, 0, iters, mu='sequence')\n",
    "                    eps_01 = eps_greedy(k, 0.01, iters, eps_0.mu.copy())\n",
    "                    eps_1 = eps_greedy(k, 0.1, iters, eps_0.mu.copy())\n",
    "\n",
    "                    # Run experiments\n",
    "                    eps_0.calculate()\n",
    "                    eps_01.calculate()\n",
    "                    eps_1.calculate()\n",
    "\n",
    "                    # Update long-term averages\n",
    "                    eps_0_rewards = eps_0_rewards + (\n",
    "                        eps_0.reward - eps_0_rewards) / (i + 1)\n",
    "                    eps_01_rewards = eps_01_rewards + (\n",
    "                        eps_01.reward - eps_01_rewards) / (i + 1)\n",
    "                    eps_1_rewards = eps_1_rewards + (\n",
    "                        eps_1.reward - eps_1_rewards) / (i + 1)\n",
    "\n",
    "                    # Average actions per episode\n",
    "                    eps_0_selection = eps_0_selection + (\n",
    "                        eps_0.k_n - eps_0_selection) / (i + 1)\n",
    "                    eps_01_selection = eps_01_selection + (\n",
    "                        eps_01.k_n - eps_01_selection) / (i + 1)\n",
    "                    eps_1_selection = eps_1_selection + (\n",
    "                        eps_1.k_n - eps_1_selection) / (i + 1)\n",
    "                    \n",
    "                    \n",
    "                    # plotting the results\n",
    "                    plt.figure(figsize=(12,8))\n",
    "                    plt.plot(eps_0_rewards, label=\"$\\epsilon=0$ (greedy)\")\n",
    "                    plt.plot(eps_01_rewards, label=\"$\\epsilon=0.01$\")\n",
    "                    plt.plot(eps_1_rewards, label=\"$\\epsilon=0.1$\")\n",
    "                    for i in range(k):\n",
    "                        plt.hlines(eps_0.mu[i], xmin=0,\n",
    "                                  xmax=iters, alpha=0.5,\n",
    "                                  linestyle=\"--\")\n",
    "                    plt.legend(bbox_to_anchor=(1.3, 0.5))\n",
    "                    plt.xlabel(\"Iterations\")\n",
    "                    plt.ylabel(\"Average Reward\")\n",
    "                    plt.title(\"Average $\\epsilon-greedy$ Rewards after \" + \n",
    "                         str(episodes) + \" Episodes\")\n",
    "                    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
